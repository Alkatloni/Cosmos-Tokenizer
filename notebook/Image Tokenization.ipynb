{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09799a-29db-46d0-85b8-6dde61373240",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python torch torchvision  # you may also need to install other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddc10a6-1b4a-49fb-955b-a83c614f2216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/PycharmProjects/Cosmos-Tokenizer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001B[0m\n",
      "downloading Cosmos-0.1-Tokenizer-CI16x16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:10<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "import os\n",
    "\n",
    "login(token=\"\", add_to_git_credential=True)\n",
    "model_names = [\n",
    "        \"Cosmos-0.1-Tokenizer-CI16x16\",\n",
    "]\n",
    "for model_name in model_names:\n",
    "    hf_repo = \"nvidia/\" + model_name\n",
    "    local_dir = \"../pretrained_ckpts/\" + model_name\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    print(f\"downloading {model_name}...\")\n",
    "    snapshot_download(repo_id=hf_repo, local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17eae9d4-e39f-403d-8b7f-e9d8d45ff779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction saved to: my_image_reconstructed.jpg\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed the Cosmos Tokenizer yet, do so first.\n",
    "# Also ensure you have a GPU runtime available, as the tokenizer requires CUDA.\n",
    "# If you cloned the repository:\n",
    "# %pip install -e /path/to/Cosmos-Tokenizer\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import importlib\n",
    "import cosmos_tokenizer.image_lib\n",
    "\n",
    "importlib.reload(cosmos_tokenizer.image_lib)\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer\n",
    "\n",
    "# 1) Specify the model name, and the paths to the encoder/decoder checkpoints.\n",
    "model_name = \"Cosmos-0.1-Tokenizer-CI16x16\"\n",
    "encoder_ckpt = f\"../pretrained_ckpts/{model_name}/encoder.jit\"\n",
    "decoder_ckpt = f\"../pretrained_ckpts/{model_name}/decoder.jit\"\n",
    "\n",
    "# 2) Load or provide the image filename you want to tokenize & reconstruct.\n",
    "input_image_path = \"../test_data/image.png\"\n",
    "\n",
    "# 3) Read the image from disk (shape = H x W x 3 in BGR). Then convert to RGB.\n",
    "original_bgr = cv2.imread(input_image_path)\n",
    "if original_bgr is None:\n",
    "    raise FileNotFoundError(f\"Could not read image file: {input_image_path}\")\n",
    "\n",
    "original_rgb = cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 4) Expand dimensions to B x H x W x C, since the ImageTokenizer expects a batch dimension\n",
    "#    in the input. (Batch size = 1 in this example.)\n",
    "input_image = np.expand_dims(original_rgb, axis=0)\n",
    "\n",
    "# 5) Create the ImageTokenizer instance with the encoder & decoder.\n",
    "#    - device=\"cuda\" uses the GPU\n",
    "#    - dtype=\"bfloat16\" expects Ampere or newer GPU (A100, RTX 30xx, etc.)\n",
    "tokenizer = ImageTokenizer(\n",
    "    checkpoint_enc=encoder_ckpt,\n",
    "    checkpoint_dec=decoder_ckpt,\n",
    "    device=\"cpu\",\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "# 6) Use the tokenizer to autoencode (encode & decode) the image.\n",
    "#    The output is a NumPy array with shape = B x H x W x C, range [0..255].\n",
    "reconstructed_image = tokenizer.forward(input_image)\n",
    "\n",
    "# 7) Extract the single image from the batch (index 0), convert to uint8.\n",
    "reconstructed_image = reconstructed_image[0].astype(np.uint8)\n",
    "\n",
    "# 8) Convert from RGB back to BGR (if you want to save using OpenCV).\n",
    "reconstructed_bgr = cv2.cvtColor(reconstructed_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# 9) Save the reconstructed image to disk.\n",
    "output_image_path = \"my_image_reconstructed.jpg\"\n",
    "cv2.imwrite(output_image_path, reconstructed_bgr)\n",
    "\n",
    "print(\"Reconstruction saved to:\", output_image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
